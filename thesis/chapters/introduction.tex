\chapter{Introduction}
\label{ch:introduction}
    When writing a compiler for a programming language,
    an important consideration is the treatment of binders and variables.
    \Fixme{Maybe briefly motivate importance of compiler correctness in general?}
    They are part of most languages and
    there are several options for representing them in a compiler,
    each with different implications for operating on and reasoning about programs.
    Often, it is possible to represent ill-formed programs
    where variables do not refer to a suitable binding.
    This makes it easy to introduce bugs when manipulating programs.

    When using a dependently typed programming language such as Agda
    \cite{Norell2008Agda},
    intrinsically typed syntax trees can be used to
    make such ill-formed programs unrepresentable.
    Using this well-known technique,
    expressions become scope- and type-correct by construction,
    allowing for a total evaluation function
    \cite{Augustsson1999WellTypedInterpreter}.
    Intrinsically typed constructions have featured in several papers,
    exploring basic operations like renaming and substitution
    \cite{Allais2018UniverseOfSyntaxes}
    as well as compilation to different target languages
    \cite[online supplementary material]{Pickard2021CalculatingDependentlyTypedCompilers}.

    At the same time, there are large classes of important transformations
    that have not yet received much attention in an intrinsically typed setting.
    Optimisations, for example, play a central role in practical compilers,
    but establishing their correctness is often not trivial,
    with ample opportunity for binding-related mistakes
    \cite{SpectorZabusky2019EmbracingFormalizationGap}
    \cite{Maclaurin2022Foil}.
    Letting the type checker keep track of invariants
    promises to remove common sources of bugs.
    A mechanised proof of semantics preservation can further increase
    confidence in the transformation's correctness.

    In return for the guarantees provided, some additional work is required.
    Program \emph{analysis} not only needs to identify optimisation opportunities,
    but potentially also provide a proof witness that the optimisation is safe,
    e.g. that some dead code is indeed unused.
    For the \emph{transformation} of the intrinsically typed program,
    the programmer then has to convince the type checker
    that type- and scope-correctness invariants are preserved,
    which can be cumbersome.
    The goal of this thesis is to understand these consequences better
    and explore different techniques for dealing with them.

    A crucial aspect is that of \emph{variable liveness}.
    Whether it is safe to apply a binding-related transformation
    usually depends on which parts of the program make use of which binding.
    We employ several ways of providing and using variable liveness information
    for program transformations.

  \paragraph{Structure}
    Chapter \ref{ch:preliminaries}
    introduces the simple expression language we will work with
    and then gives some background information on program analysis and transformation,
    as well as different binding representations and their pitfalls.

    In chapter \ref{ch:de-bruijn} we start by
    showing a typical intrinsically typed de Bruijn representation of a simple expression language.
    We then explain thinnings and motivate their application to computing variable liveness.
    Equipped with these tools,
    we implement dead binding elimination and let-sinking,
    first on the standard de Bruijn representation,
    later on a syntax tree annotated with the results of live variable analysis.
    We prove that both versions of dead binding elimination preserve semantics.

    Chapter \ref{ch:co-de-bruijn} continues the development by showing that variable liveness information
    can serve as the main mechanism for representing bindings, as witnessed by
    McBride's co-de-Bruijn representation
    \cite{McBride2018EveryBodysGotToBeSomewhere}.
    After explaining how co-de-Bruijn terms work and can be constructed from de Bruijn terms,
    we again implement dead binding elimination and prove it correct.
    Finally, we manage to implement dead-binding elimination, but encounter several complications
    and struggle with the proof of correctness.

    In chapter \ref{ch:generic-co-de-bruijn}, we explain the basic ideas of syntax-generic programming
    as presented by Allais et al.
    \cite{Allais2018UniverseOfSyntaxes}
    and extend it with basic support for co-de-Bruijn representation.
    This allows us to generically convert between de Bruijn and co-de-Bruijn representation
    and perform dead binding elimination.
    \Fixme{Can be fleshed out a little bit once everything stabilised. Also list main points of Discussion chapter.}

  \paragraph{Contributions}
    Our main contributions are:
    \begin{itemize}
      \item an implementation of (strongly) live variable analysis resulting in annotated intrinsically typed syntax trees
      \item an implementation of dead binding elimination and let-sinking on intrinsically typed syntax trees of three different flavours: de Bruijn, annotated de Bruijn, and co-de-Bruijn
      \item proofs of correctness (preservation of semantics) for the implementations of dead binding elimination
      \item an incomplete proof of correctness for co-de-Bruijn let-sinking, with an explanation of the main challenges
      \item a generic interpretation of the syntax descriptions presented by Allais et al. \cite{Allais2018UniverseOfSyntaxes} into co-de-Bruijn terms
      \item syntax-generic conversion between de Bruijn and co-de-Bruijn terms
      \item a syntax-generic implementation of dead binding elimination on co-de-Bruijn terms
    \end{itemize}
    \Fixme{Add forward references onces structure is stable.}
    The source code is available online%
    \footnote{\url{https://git.science.uu.nl/m.h.heinzel/correct-optimisations}}.

  \paragraph{Ethics review}
    The Ethics and Privacy Quick Scan of the Utrecht University Research Institute of Information and Computing Sciences was conducted (see Appendix \ref{app:ethics-quick-scan}).
    It classified this research as low-risk with no fuller ethics review or privacy assessment required.
