\chapter{Preliminaries}
\label{ch:preliminaries}
    As a running example, we will consider a simple expression language
    based on the $\lambda$-calculus
    \cite{Barendregt1985LambdaCalculus}.
    On top of variables with names $\{ x, y, z, a, b, c, f, g, \ldots \}$, function application and $\lambda$-abstraction,
    we add let-bindings, primitive values $v \in \mathbb{B} \cup \mathbb{N}$ (with $\mathbb{B} = \{ \ValTrue , \ValFalse \}$) and a binary addition operator.
    Since we are primarily concerned with variables and binders,
    the choice of possible values and primitive operations on them is mostly arbitrary and can be extended easily.
    \begin{align*}
      P, Q ::=&\ x
      \\ \big|&\ P\ Q
      \\ \big|&\ \lambda x.\ P
      \\ \big|&\ \textbf{let } x = P \textbf{ in } Q
      \\ \big|&\ v
      \\ \big|&\ P + Q
    \end{align*}

    To reduce the number of required parentheses,
    we give function application the highest
    and let-bindings the lowest precedence.

    Let-bindings allow to bind a declaration $P$ to a variable $x$.
    While any let-binding $\Let{x} P \In Q$ can be emulated
    using an immediately applied $\lambda$-abstraction $(\lambda x.\ Q)\ P$,
    they are very common and can benefit
    from transformations that target them specifically.
    We omit further constructs such as branching operators,
    recursive bindings or a fixpoint operator,
    but discuss some potential additions and their implications
    at the end (section \ref{sec:further-work}).

\section{Program Analysis and Transformation}
\label{sec:program-transformations}
    We mainly consider transformations aimed at optimising functional programs.
    A large number of program analyses and optimisations are presented in the literature
    \cite{Nielson1999PrinciplesProgramAnalysis}
    \cite{Santos1995CompilationByTransformation}
    and used in production compilers such as the Glorious Haskell Compiler (GHC).
    We generally focus on those transformations that deal with variable binders,
    such as
    \emph{inlining},
    \emph{let-floating},
    \emph{common subexpression elimination} and
    \emph{dead binding elimination},
    which is explained below.
  \paragraph{Dead Binding Elimination}
    An expression is not forced to make use of the whole context to which it has access.
    \Fixme{explain ``context'' somewhere?}
    Specifically, a let-binding introduces a new variable, but it might never be used
    in the body.
    Consider for example the following expression:
    \begin{align*}
      &\Let{x} 42 \In            \\
      &\ \ \Let{y} x + 6 \In     \\
      &\ \ \ \ \Let{z} y + 7 \In \\
      &\ \ \ \ \ \ x
    \end{align*}
    Here, the binding for $z$ is clearly unused, as the variable never occurs in the body.
    Such dead bindings can be identified by \emph{live variable analysis}
    and consequently be removed.

    Note that $y$ is not needed either: Removing $z$ will make $y$ unused.
    Therefore, multiple iterations of live variable analysis and binding elimination might be required.
    Alternatively, \emph{strongly live variable analysis} can achieve the same result in a single pass
    by only considering variables to be live
    if they are used in declarations of variables that are live themselves.
  \paragraph{Let-sinking}
    Even when a binding cannot be removed,
    it can still be beneficial to move it to a different location.
    Several such strategies have for example been described and evaluated
    in the context of lazy functional programs
    \cite{Jones1996LetFloating}.

    Of those, we will focus on the \emph{let-sinking} transformation
    (called let-floating in the paper).
    Generally, the further inward a let binding is moved, the better:
    Further optimisations might get unlocked and in the presence of branching,
    the binding it might never be executed.
    Of course, we must be careful that the binding remains in scope
    for all of the variable's occurrences
    and consider some exceptions to the heuristic of pushing as far as possible.
    We do not want to duplicate the binding
    or move it inside a $\lambda$-abstraction, which duplicates work
    if evaluated multiple times.
    Let us look at what this means when let-sinking $x$ in the following example with free variables $a$ and $f$:

    \begin{align*}
      \Let{x} f\ 42 \In b\ (a + 1)\ (f\ x + x)
    \end{align*}
    \begin{align*}
      \Downarrow
    \end{align*}
    \begin{align*}
      b\ (a + 1)\ (\Let{x} f\ 42 \In f\ x + x)
    \end{align*}

    Interestingly, let-sinking also covers a central part of \emph{inlining}.
    When a variable only occurs once (and would thus benefit from inlining),
    the binding will be moved inwards until it reaches the single occurence,
    which can then be replaced by the binding's declaration.

    \begin{align*}
      &\Let{x} f\ 42 \In          \\
      &\ \ \Let{y} f\ 43 \In      \\
      &\ \ \ \ f\ y + (y + x)
    \end{align*}
    \begin{align*}
      \Downarrow
    \end{align*}
    \begin{align*}
      &\Let{y} f\ 43 \In      \\
      &\ \ f\ y + (y + \Let{x} f\ 42 \In x)
    \end{align*}
    \begin{align*}
      \Downarrow
    \end{align*}
    \begin{align*}
      &\Let{y} f\ 43 \In      \\
      &\ \ f\ y + (y + f\ 42)
    \end{align*}

\section{Binding Representations}
\label{sec:binding-representations}
    The notation used so far treats variables as letters, or more generally strings.
    This is how humans usually write programs
    and makes it fairly natural to match a variable with its binding.
    For representing variables in a compiler or mechanised proof, however,
    different tradeoffs apply.
  \paragraph{Explicit names}
    It is quite common to use strings for variables in practical compilers,
    but comes with several disadvantages.
    For example, additional work is necessary
    if we want the equality of expressions to be independent of the specific variable names chosen
    (\emph{$\alpha$-equivalence}).
    Also, there are pitfalls like variable shadowing and variable capture during substitution,
    requiring the careful application of variable renamings
    \cite{Barendregt1985LambdaCalculus}.
    Consider for example the following expression, where $x$ is a free variable:

    \begin{align*}
      \Let{y} x + 1 \In \Lam{x} (x + y)
    \end{align*}

    Naively inlining $y$ here causes $x$ to be captured
    by the $\lambda$-abstraction, incorrectly resulting in the program
    $\Lam{x} (x + (x + 1))$.

    There have been various approaches to help compiler writers
    maintain the relevant invariants,
    such as GHC's \emph{rapier} \cite{Jones2002GHCInliner},
    but these are generally still error-prone.
    The developers of Dex for example used the rapier,
    but introduced multiple binding-related compiler bugs,
    leading them to create \emph{the foil}
    to ``make it harder to poke your eye out''
    \cite{Maclaurin2022Foil}.

  \paragraph{de Bruijn Indices}
    With \emph{de Bruijn indices}
    \cite{DeBruijn1972NamelessIndices},
    we instead adopt a \emph{nameless} representation.
    Each variable is represented as a natural number,
    counting the number of nested bindings between variable occurrence and its binding:
    $\DeBruijn{0}$ refers to the innermost binding, $\DeBruijn{1}$ to the next-innermost etc.
    If we adapt the syntax for let-bindings to omit the unnecessary variable name,
    the example expression from dead binding elimination is represented as follows:

    \begin{align*}
      &\Let{x} 42 \In            & &\LetB 42 \In                       \\
      &\ \ \Let{y} x + 6 \In     & &\ \ \LetB \DeBruijn{0} + 6 \In     \\
      &\ \ \ \ \Let{z} y + 7 \In & &\ \ \ \ \LetB \DeBruijn{0} + 7 \In \\
      &\ \ \ \ \ \ x             & &\ \ \ \ \ \ \DeBruijn{2}
    \end{align*}

    This makes $\alpha$-equivalence of expressions trivial and avoids variable capture,
    but there are still opportunities for mistakes during transformations.
    Adding or removing a binding
    requires us to traverse the binding's body and add or subtract 1 from all its free variables.
    We can see this in our example when removing the innermost (unused) let-binding.
    If we leave the variable $\DeBruijn{2}$ untouched,
    it will not refer to the declaration $42$ anymore,
    but become a free variable:

    \begin{align*}
      &\LetB 42 \In                   \\
      &\ \ \LetB \DeBruijn{0} + 6 \In \\
      &\ \ \ \ \DeBruijn{2}\ \ \ \textit{-- incorrect, should be 1}
    \end{align*}

    While useful for machines, de Bruijn representation can be unintuitive
    for humans to reason about.
    This can be alleviated by formally describing the necessary invariants
    and using tooling to make sure they are upheld.
    Intrinsically typed de Bruijn representation is one
    possible way to achieve that, as demonstrated in section
    \ref{sec:de-bruijn-intrinsically-typed}.

  \paragraph{Other Representations}
    There are many other techniques%
    \footnote{
    There is an introductory blogpost by Jesper Cockx
    \cite{Cockx2021RepresentationsBinding}
    comparing several approaches and their properties using Agda.
    }
    such as higher-order abstract syntax
    \cite{Pfenning1988HOAS}
    and also combinations of multiple techniques, e.g. the locally nameless representation
    \cite{Chargueraud2011LocallyNameless}.

    Another nameless option we only brief mention here is the
    \emph{co-de-Bruijn representation}
    \cite{McBride2018EveryBodysGotToBeSomewhere}.
    It does not only admit a trivial $\alpha$-equivalence,
    but its terms are also unchanged by adding or removing bindings
    in its context.
    On the other hand,
    it is even harder for humans to comprehend than de Bruijn syntax.
    McBride writes that
    ``only a fool would attempt to enforce the co-de-Bruijn invariants
    without support from a typechecker''
    and makes heavy use of Agda's dependent type system.
    We follow his approach closely, as shown in section
    \ref{sec:co-de-bruijn-intrinsically-typed}.
